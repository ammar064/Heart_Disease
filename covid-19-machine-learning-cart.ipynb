{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/amarmoibrahim964/covid-19-machine-learning-cart?scriptVersionId=142881288\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Covid 19 Machine Learning Project CART","metadata":{}},{"cell_type":"markdown","source":"![/kaggle/input/corona/corona.jpg](https://images.pexels.com/photos/4031867/pexels-photo-4031867.jpeg?auto=compress&cs=tinysrgb&w=1260&h=750&dpr=1)","metadata":{}},{"cell_type":"markdown","source":"# Introduction\n\n**Coronavirus** disease 2019 (COVID-19) is a contagious disease caused by the virus severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). The first known case was identified in Wuhan, China, in December 2019.The disease quickly spread worldwide, resulting in the COVID-19 pandemic.\n\nMachine learning (ML) is a subfield of artificial intelligence, which is used to perform complex tasks in a way that is similar to how humans solve problems. ML starts with data numbers, photos, or text, like bank transactions, repair records, time series data from sensors or reports and predicts the\ncorresponding result. There are two ways in which the machine learns. It could be supervised ML,\nunsupervised ML or reinforcement ML. Supervised ML could be in the form of CART algorithm, which\nis used in this project for regression analysis. CART can be applied to predict a categorical target variable producing a classification tree, or continuous target variable producing a regression tree. We have used this method to explain the statistics of corona cases around the world depending on region specificities.\n","metadata":{}},{"cell_type":"markdown","source":"# Method\n\nTo set up our model, we rely on two main methods, **the classification and regression tree ( CART)** and\nto assess our performance, we utilized **( K fold cross validation)** , but we will shed light on the cross\nvalidation in details when we evaluate our model.\n\n ","metadata":{}},{"cell_type":"markdown","source":"![/kaggle/input/tree-reg2/tree reg.jpg](https://www.analyticssteps.com/backend/media/thumbnail/2578400/1990226_1626945689_CART%20algorithmArtboard%201%20copy.jpg)","metadata":{}},{"cell_type":"markdown","source":"Regression analysis of the decision tree type is used in predictive models to predict a continuous target variable in supervised learning. The fundamental idea is to divide the data set into more manageable sections. Both linear and non-linear relationships can be studied using this non-parametric approach.\nDecision trees come in two primary variants: categorical (classification trees) and numerical variables\n(regression tree). Numerical or categorical explanatory variables are also acceptable. A numerical label\nis estimated via regression. This implies that the possible values for the output are unlimited ( Glenn\nDe'ath, et al 2000) . We utilize decision tree regression in the corona dataset because it contains\nnumerical variable.\n\n\nOn the one hand, the advantage of decision trees is that they are easy to understand. The decision tree\ncan tolerate missing data and preserve accuracy, and it doesn't require extensive data preparation such\nas normalization or standardization ( Glenn De'ath, et al 2000 ) . It can simulate nonlinear inputâ€“output\nrelationships. On the other hand, the disadvantage of decision trees, is that they can be biased towards\nfeatures with many levels, which makes it more likely that they will be chosen as splits in the tree.\nAdditionally, the decision tree algorithm chooses the best split at each step without taking the effects\n2\nof future splits into account. And changing the data slightly can result in insignificant changes to the\nstructure.","metadata":{}},{"cell_type":"markdown","source":"# Application\n1. **Installing the packages and importing the dataset** For our project we chose Python language through installing packages, libraries and loading the dataset. We require in our model packages such as Pyreadr to read data in python, numPy (Numerical Python library), matplotlib (Python data visualization library), seaborn (Python advanced data visualization library), Scikit-learn (Python machine learning library)\n","metadata":{}},{"cell_type":"code","source":"! pip install pyreadr\n\n","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-08-30T13:45:34.288653Z","iopub.execute_input":"2023-08-30T13:45:34.289076Z","iopub.status.idle":"2023-08-30T13:46:01.912104Z","shell.execute_reply.started":"2023-08-30T13:45:34.289047Z","shell.execute_reply":"2023-08-30T13:46:01.910702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pyreadr\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2023-08-30T13:46:01.914326Z","iopub.execute_input":"2023-08-30T13:46:01.914715Z","iopub.status.idle":"2023-08-30T13:46:01.921188Z","shell.execute_reply.started":"2023-08-30T13:46:01.914681Z","shell.execute_reply":"2023-08-30T13:46:01.920097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = pyreadr.read_r('/kaggle/input/corona-data/CoronaData (1).rdata')\ndf = result[\"df\"]\ndf","metadata":{"execution":{"iopub.status.busy":"2023-08-30T13:46:01.922526Z","iopub.execute_input":"2023-08-30T13:46:01.922883Z","iopub.status.idle":"2023-08-30T13:46:01.995116Z","shell.execute_reply.started":"2023-08-30T13:46:01.922855Z","shell.execute_reply":"2023-08-30T13:46:01.99323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2023-08-30T13:46:01.999161Z","iopub.execute_input":"2023-08-30T13:46:01.99953Z","iopub.status.idle":"2023-08-30T13:46:02.018277Z","shell.execute_reply.started":"2023-08-30T13:46:01.999501Z","shell.execute_reply":"2023-08-30T13:46:02.017084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For our project we chose Python language through installing packages, libraries and loading the\ndataset. We require in our model packages such as Pyreadr to read data in python, numPy (Numerical\nPython library), matplotlib (Python data visualization library), seaborn (Python advanced data\nvisualization library), Scikit-learn (Python machine learning library). When we are exploring our dataset,\nwe find 14 columns and 2227 rows in our data frame which contain 10 columns, numeric float 64, and\n4 string variables. Then columns of our data include location, year, month, new_cases_pm, iso_code,\ncontinent, population_density, median_age, aged_65_older, gdp_per_capita, extreme_poverty,\nlife_expectancy, human development index and date.","metadata":{}},{"cell_type":"markdown","source":"**2.Preparation and cleaning the data**\n","metadata":{}},{"cell_type":"markdown","source":"Before using the data, we need to prepare our data to be used in the model. That means removing the\nduplicate data and dealing with the null values. First, we filtered our data by choosing the data of two\nmonths (January and July 2021) and then only selecting the entries whose dependent parameter stays\neither in the lowest 40% or the highest 40%.","metadata":{}},{"cell_type":"code","source":"#Get same Statistical information (mean & stander error & max & Min)\ndf.describe()","metadata":{"execution":{"iopub.status.busy":"2023-08-30T13:46:02.019676Z","iopub.execute_input":"2023-08-30T13:46:02.020105Z","iopub.status.idle":"2023-08-30T13:46:02.061792Z","shell.execute_reply.started":"2023-08-30T13:46:02.020075Z","shell.execute_reply":"2023-08-30T13:46:02.060923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns","metadata":{"execution":{"iopub.status.busy":"2023-08-30T13:46:02.063196Z","iopub.execute_input":"2023-08-30T13:46:02.063743Z","iopub.status.idle":"2023-08-30T13:46:02.070486Z","shell.execute_reply.started":"2023-08-30T13:46:02.063707Z","shell.execute_reply":"2023-08-30T13:46:02.069438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['location'].unique()","metadata":{"execution":{"iopub.status.busy":"2023-08-30T13:46:02.071922Z","iopub.execute_input":"2023-08-30T13:46:02.072302Z","iopub.status.idle":"2023-08-30T13:46:02.085398Z","shell.execute_reply.started":"2023-08-30T13:46:02.072275Z","shell.execute_reply":"2023-08-30T13:46:02.084094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['year'].unique()","metadata":{"execution":{"iopub.status.busy":"2023-08-30T13:46:02.087097Z","iopub.execute_input":"2023-08-30T13:46:02.087762Z","iopub.status.idle":"2023-08-30T13:46:02.104932Z","shell.execute_reply.started":"2023-08-30T13:46:02.087731Z","shell.execute_reply":"2023-08-30T13:46:02.104109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['month'].unique()","metadata":{"execution":{"iopub.status.busy":"2023-08-30T13:46:02.105955Z","iopub.execute_input":"2023-08-30T13:46:02.106936Z","iopub.status.idle":"2023-08-30T13:46:02.124473Z","shell.execute_reply.started":"2023-08-30T13:46:02.106886Z","shell.execute_reply":"2023-08-30T13:46:02.123659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we filter the data with  January 2021 and July 2021\n\ndf1=df[df['year']==2021.0]\ndf=df1[(df1['month']==1.0)|(df1['month']==7.0 )]\ndf","metadata":{"execution":{"iopub.status.busy":"2023-08-30T13:46:02.128533Z","iopub.execute_input":"2023-08-30T13:46:02.128949Z","iopub.status.idle":"2023-08-30T13:46:02.16366Z","shell.execute_reply.started":"2023-08-30T13:46:02.128918Z","shell.execute_reply":"2023-08-30T13:46:02.162443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#code in low and high (lowest 40%, highest 40%)\n# get highest 40%\ndf_high=df[df['new_cases_pm'] >=df['new_cases_pm'].quantile(0.60)]\n\n\n# get Lowest 40%\ndf_low=df[df['new_cases_pm'] <=df['new_cases_pm'].quantile(0.40)]\n\n","metadata":{"execution":{"iopub.status.busy":"2023-08-30T13:46:02.165399Z","iopub.execute_input":"2023-08-30T13:46:02.165824Z","iopub.status.idle":"2023-08-30T13:46:02.176019Z","shell.execute_reply.started":"2023-08-30T13:46:02.165787Z","shell.execute_reply":"2023-08-30T13:46:02.174939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df40=df[ (df['new_cases_pm'] <=df['new_cases_pm'].quantile(0.40) ) | (df['new_cases_pm'] >=df['new_cases_pm'].quantile(0.60) )  ]\ndf40","metadata":{"execution":{"iopub.status.busy":"2023-08-30T13:46:02.177464Z","iopub.execute_input":"2023-08-30T13:46:02.177819Z","iopub.status.idle":"2023-08-30T13:46:02.218536Z","shell.execute_reply.started":"2023-08-30T13:46:02.177789Z","shell.execute_reply":"2023-08-30T13:46:02.217437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**cleaning data** : Check For Duplicate Data & Check Missing Values In","metadata":{}},{"cell_type":"code","source":"##Check For Duplicate Data\n\ndup=df40.duplicated().any()\nprint(\"Any duplicate Value?\",dup)","metadata":{"execution":{"iopub.status.busy":"2023-08-30T13:46:02.219857Z","iopub.execute_input":"2023-08-30T13:46:02.220156Z","iopub.status.idle":"2023-08-30T13:46:02.231463Z","shell.execute_reply.started":"2023-08-30T13:46:02.220132Z","shell.execute_reply":"2023-08-30T13:46:02.230622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Check Missing Values In The Dataset\n\ndf40.isnull().sum()\n\npre_missing=df40.isnull().sum()*100/len(df)\npre_missing\n","metadata":{"execution":{"iopub.status.busy":"2023-08-30T13:46:02.233025Z","iopub.execute_input":"2023-08-30T13:46:02.233649Z","iopub.status.idle":"2023-08-30T13:46:02.24837Z","shell.execute_reply.started":"2023-08-30T13:46:02.233581Z","shell.execute_reply":"2023-08-30T13:46:02.247031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(df40.isnull(),cmap='viridis',cbar=True,yticklabels=False)\nplt.title(\"Missing Data\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-30T13:46:02.251551Z","iopub.execute_input":"2023-08-30T13:46:02.25199Z","iopub.status.idle":"2023-08-30T13:46:02.58762Z","shell.execute_reply.started":"2023-08-30T13:46:02.251948Z","shell.execute_reply":"2023-08-30T13:46:02.586714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We did not find any duplicate data but we detected some Null entries in independent variable. This\nincludes 3% of (Population density, human development index, aged_65_older), 5% in (GDP per capita)\nand 20% in (extreme poverty). To address this issue, we can either replace the missing data with the\nmean value if it is numerical, or just ignore and remove it. As the number of missing data was not\nsignificant, we decided to remove entries with missing data. For that, we needed to drop out\n(extreme_poverty) column first, because it has around 20% of missing entries. After cleaning our data.","metadata":{}},{"cell_type":"code","source":"#Drop the column extreme_poverty because it has 19.53125% miss data\n\ndf40.drop('extreme_poverty',axis=1,inplace=True)","metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-08-30T13:46:02.588938Z","iopub.execute_input":"2023-08-30T13:46:02.589886Z","iopub.status.idle":"2023-08-30T13:46:02.597183Z","shell.execute_reply.started":"2023-08-30T13:46:02.58985Z","shell.execute_reply":"2023-08-30T13:46:02.595851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Drop All the Missing Values\ndf40.dropna(how='any',inplace=True)\n","metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-08-30T13:46:02.598557Z","iopub.execute_input":"2023-08-30T13:46:02.598925Z","iopub.status.idle":"2023-08-30T13:46:02.612926Z","shell.execute_reply.started":"2023-08-30T13:46:02.598896Z","shell.execute_reply":"2023-08-30T13:46:02.611654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df40","metadata":{"execution":{"iopub.status.busy":"2023-08-30T13:46:02.614366Z","iopub.execute_input":"2023-08-30T13:46:02.614728Z","iopub.status.idle":"2023-08-30T13:46:02.650541Z","shell.execute_reply.started":"2023-08-30T13:46:02.6147Z","shell.execute_reply":"2023-08-30T13:46:02.649113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df40['continent'].unique()","metadata":{"execution":{"iopub.status.busy":"2023-08-30T13:46:02.65192Z","iopub.execute_input":"2023-08-30T13:46:02.652336Z","iopub.status.idle":"2023-08-30T13:46:02.670905Z","shell.execute_reply.started":"2023-08-30T13:46:02.652305Z","shell.execute_reply":"2023-08-30T13:46:02.669858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Encoding Categorical Data","metadata":{}},{"cell_type":"markdown","source":"Before creating our model, we needed to determine our desired predictors (independent variables)\nand the target (dependent variable), where all of them must be numerical values. For that, we need to\nassign numbers to some string parameters. Specifically, we created another column (locat_SOrN) as a\ncategorical value (North or South). We assigned North America, Europe, and Asia to North and the rest\nof the world to South. Then we created another column (North_or_South) as the Dummy variable; for\nNorth taking the value of 1 and for South the value of 0. Finally, we dropped out all categorical data\nand unwanted columns and kept only numerical variables. In addition, we used the two months\n(January and July) as a dummy variable.","metadata":{}},{"cell_type":"code","source":"#Create a Categorical North and South\ndef locat(locat):\n    if locat in ['North America','Europe','Asia' ]:\n        return \"North\"\n    else:\n        return \"South\"\n\ndf40['locat_SOrN']=df40['continent'].apply(locat)\ndf40","metadata":{"execution":{"iopub.status.busy":"2023-08-30T13:46:02.67238Z","iopub.execute_input":"2023-08-30T13:46:02.673136Z","iopub.status.idle":"2023-08-30T13:46:02.708886Z","shell.execute_reply.started":"2023-08-30T13:46:02.673099Z","shell.execute_reply":"2023-08-30T13:46:02.707694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"###create Dummy Variable\nx=df40['locat_SOrN'].map({'North':1,'South':0})\ndf40.insert(14,'North_or_South ',x)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-30T13:46:02.710128Z","iopub.execute_input":"2023-08-30T13:46:02.710548Z","iopub.status.idle":"2023-08-30T13:46:02.723107Z","shell.execute_reply.started":"2023-08-30T13:46:02.710516Z","shell.execute_reply":"2023-08-30T13:46:02.722016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"###create Dummy Variable\ndef season(season):\n    if season == 1.0:\n        return  1\n    else:\n        return  0\n\ndf40['Season']=df40['month'].apply(season)\ndf40","metadata":{"execution":{"iopub.status.busy":"2023-08-30T13:46:02.724665Z","iopub.execute_input":"2023-08-30T13:46:02.724989Z","iopub.status.idle":"2023-08-30T13:46:02.762099Z","shell.execute_reply.started":"2023-08-30T13:46:02.724963Z","shell.execute_reply":"2023-08-30T13:46:02.761019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#3Check the correlation\n\n#corr=df40.corr()\n#corr.style.background_gradient(cmap='coolwarm',axis=None)\n\nplt.figure(figsize=(16, 6))\nheatmap = sns.heatmap(df40.corr(), vmin=-1, vmax=1, annot=True)\nheatmap.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12);","metadata":{"execution":{"iopub.status.busy":"2023-08-30T13:46:02.76374Z","iopub.execute_input":"2023-08-30T13:46:02.764178Z","iopub.status.idle":"2023-08-30T13:46:03.572496Z","shell.execute_reply.started":"2023-08-30T13:46:02.764141Z","shell.execute_reply":"2023-08-30T13:46:03.571669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8, 12))\nheatmap = sns.heatmap(df40.corr()[['new_cases_pm']].sort_values(by='new_cases_pm', ascending=False), vmin=-1, vmax=1, annot=True, cmap='BrBG')\nheatmap.set_title('Features Correlating with new_cases_pm', fontdict={'fontsize':18}, pad=16);","metadata":{"execution":{"iopub.status.busy":"2023-08-30T13:46:03.573718Z","iopub.execute_input":"2023-08-30T13:46:03.574192Z","iopub.status.idle":"2023-08-30T13:46:04.058976Z","shell.execute_reply.started":"2023-08-30T13:46:03.574164Z","shell.execute_reply":"2023-08-30T13:46:04.057694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#we need only the nomric columns\n# Drop unuseless columns , we need only the nomric columns\n\ndf_f=df40.drop(columns=['continent','iso_code','location','date','location','year','locat_SOrN','month'])\n","metadata":{"execution":{"iopub.status.busy":"2023-08-30T13:46:04.060635Z","iopub.execute_input":"2023-08-30T13:46:04.061158Z","iopub.status.idle":"2023-08-30T13:46:04.067473Z","shell.execute_reply.started":"2023-08-30T13:46:04.061106Z","shell.execute_reply":"2023-08-30T13:46:04.066591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**3.Creating the model**","metadata":{}},{"cell_type":"markdown","source":"a) Splitting variables into Predictors (x) and Response variable (y).\nWe split data into Predictors (x) as(month, population_density, median_age, aged_65_older, GDP_ per\ncapita, life_expectancy, human _development index, North or South) and target (y) as (new cases pm).","metadata":{}},{"cell_type":"code","source":"#1 .Split into on Predictors (x) and Respondse variable (y)\nx=df_f.drop('new_cases_pm',axis=1)\ny=df_f['new_cases_pm']","metadata":{"execution":{"iopub.status.busy":"2023-08-30T13:46:04.069431Z","iopub.execute_input":"2023-08-30T13:46:04.069784Z","iopub.status.idle":"2023-08-30T13:46:04.087712Z","shell.execute_reply.started":"2023-08-30T13:46:04.069755Z","shell.execute_reply":"2023-08-30T13:46:04.08689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#2.Splitting the dataset into the Training set and Test set\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20,random_state=42)","metadata":{"execution":{"iopub.status.busy":"2023-08-30T13:46:04.088925Z","iopub.execute_input":"2023-08-30T13:46:04.089421Z","iopub.status.idle":"2023-08-30T13:46:04.107086Z","shell.execute_reply.started":"2023-08-30T13:46:04.089372Z","shell.execute_reply":"2023-08-30T13:46:04.106168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#3.Fit Model with Training Data\n# import the regressor\nfrom sklearn.tree import DecisionTreeRegressor\n\n# create a regressor object\nmodel = DecisionTreeRegressor()\n\n# fit the regressor with X and Y data\nmodel.fit(x_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2023-08-30T13:46:04.111484Z","iopub.execute_input":"2023-08-30T13:46:04.112083Z","iopub.status.idle":"2023-08-30T13:46:04.396577Z","shell.execute_reply.started":"2023-08-30T13:46:04.11205Z","shell.execute_reply":"2023-08-30T13:46:04.395495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Ploting the tree decicion\nfrom sklearn import tree\nfig=plt.figure(figsize=(43,14))\ntree.plot_tree(model,filled=True,rounded=True,max_depth=3,fontsize=20)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-30T13:46:04.398098Z","iopub.execute_input":"2023-08-30T13:46:04.398529Z","iopub.status.idle":"2023-08-30T13:46:05.892726Z","shell.execute_reply.started":"2023-08-30T13:46:04.39849Z","shell.execute_reply":"2023-08-30T13:46:05.891746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**4. Evaluating the model using cross-validated metrics**","metadata":{}},{"cell_type":"markdown","source":"the easiest method to enhance the system's performance without sacrificing too much is to verify it using a tiny portion of the training data, since this will give us an indication of the model's capacity to predict unknown data.\n\n \n\nK-fold cross-validation is a prominent type of cross-validation approach in which, for example, if k=5, 4 folds are used for training and 1 fold is used for testing, and this process repeats until all folds have a chance to be the test set one by one.","metadata":{}},{"cell_type":"markdown","source":"![/kaggle/input/cross-validation-img12/cross-validation.png](https://static.javatpoint.com/tutorial/machine-learning/images/cross-validation.png)","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nnp.random.seed(42)\nScore=cross_val_score(model,x_train,y_train,cv=5)\n\nscore_mean=-Score.mean()\n\nprint(\"%0.2f accuracy with a standard deviation of %0.2f \" %(score_mean, Score.std()))","metadata":{"execution":{"iopub.status.busy":"2023-08-30T13:46:05.894043Z","iopub.execute_input":"2023-08-30T13:46:05.894876Z","iopub.status.idle":"2023-08-30T13:46:05.929595Z","shell.execute_reply.started":"2023-08-30T13:46:05.894836Z","shell.execute_reply":"2023-08-30T13:46:05.928572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By applying\nthis method to our model, we get these result of accuracy 70% of with a standard deviation of 0.94.Consequently, we can say that this method is considered the most proficient way to estimate our performance of machine\nlearning because it ensures that every observation has the opportunity to be clear in training and\ntesting the mode.","metadata":{}},{"cell_type":"markdown","source":"# Conclusion\nTo recapitulate, Machine learning is a powerful method to explain large dataset and create models\nbased on statistics. However, the data work up and classification is a very important step that can affect the final model. The classification and regression tree (CART) and cross-validation are the most\nprominent parts for evaluating the performance of our model, especially if we need to alleviate\noverfitting issue.","metadata":{}}],"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}}